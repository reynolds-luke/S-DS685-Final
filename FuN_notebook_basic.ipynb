{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41eea6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luk27\\anaconda3\\envs\\interp\\Lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luk27\\anaconda3\\envs\\interp\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP 00010 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.000 | steps=100 | loss_w=-0.021 | loss_m_pg=+0.011\n",
      "EP 00020 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.004 | steps=100 | loss_w=-0.019 | loss_m_pg=+0.030\n",
      "EP 00030 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.002 | steps=100 | loss_w=-0.030 | loss_m_pg=+0.005\n",
      "EP 00040 | ⟨R_ext⟩=0.005 | ⟨R_int⟩=0.003 | steps= 72 | loss_w=-0.059 | loss_m_pg=+0.018\n",
      "EP 00050 | ⟨R_ext⟩=0.082 | ⟨R_int⟩=-0.032 | steps= 11 | loss_w=+0.096 | loss_m_pg=-0.253\n",
      "EP 00060 | ⟨R_ext⟩=0.241 | ⟨R_int⟩=-0.066 | steps=  4 | loss_w=-0.003 | loss_m_pg=+0.000\n",
      "EP 00070 | ⟨R_ext⟩=0.003 | ⟨R_int⟩=-0.001 | steps= 85 | loss_w=-0.134 | loss_m_pg=+0.010\n",
      "EP 00080 | ⟨R_ext⟩=0.014 | ⟨R_int⟩=0.006 | steps= 44 | loss_w=-0.259 | loss_m_pg=-0.009\n",
      "EP 00090 | ⟨R_ext⟩=0.008 | ⟨R_int⟩=-0.011 | steps= 59 | loss_w=-0.378 | loss_m_pg=-0.030\n",
      "EP 00100 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.003 | steps=100 | loss_w=+0.041 | loss_m_pg=-0.023\n",
      "EP 00110 | ⟨R_ext⟩=0.004 | ⟨R_int⟩=-0.004 | steps= 80 | loss_w=+0.116 | loss_m_pg=-0.006\n",
      "EP 00120 | ⟨R_ext⟩=0.023 | ⟨R_int⟩=0.001 | steps= 31 | loss_w=-0.024 | loss_m_pg=-0.031\n",
      "EP 00130 | ⟨R_ext⟩=0.116 | ⟨R_int⟩=0.009 | steps=  8 | loss_w=-0.195 | loss_m_pg=+0.000\n",
      "EP 00140 | ⟨R_ext⟩=0.020 | ⟨R_int⟩=0.012 | steps= 34 | loss_w=-0.324 | loss_m_pg=-0.034\n",
      "EP 00150 | ⟨R_ext⟩=0.019 | ⟨R_int⟩=0.010 | steps= 36 | loss_w=-0.382 | loss_m_pg=+0.001\n",
      "EP 00160 | ⟨R_ext⟩=0.044 | ⟨R_int⟩=-0.022 | steps= 19 | loss_w=-0.338 | loss_m_pg=-0.009\n",
      "EP 00170 | ⟨R_ext⟩=0.015 | ⟨R_int⟩=0.004 | steps= 42 | loss_w=+0.020 | loss_m_pg=+0.047\n",
      "EP 00180 | ⟨R_ext⟩=0.006 | ⟨R_int⟩=-0.007 | steps= 68 | loss_w=-0.148 | loss_m_pg=-0.012\n",
      "EP 00190 | ⟨R_ext⟩=0.068 | ⟨R_int⟩=-0.014 | steps= 13 | loss_w=-0.422 | loss_m_pg=-0.092\n",
      "EP 00200 | ⟨R_ext⟩=0.019 | ⟨R_int⟩=0.013 | steps= 36 | loss_w=-0.453 | loss_m_pg=+0.014\n",
      "EP 00210 | ⟨R_ext⟩=0.102 | ⟨R_int⟩=-0.028 | steps=  9 | loss_w=-0.264 | loss_m_pg=+0.000\n",
      "EP 00220 | ⟨R_ext⟩=0.008 | ⟨R_int⟩=0.001 | steps= 59 | loss_w=-0.095 | loss_m_pg=-0.003\n",
      "EP 00230 | ⟨R_ext⟩=0.082 | ⟨R_int⟩=0.045 | steps= 11 | loss_w=-0.464 | loss_m_pg=-0.000\n",
      "EP 00240 | ⟨R_ext⟩=0.009 | ⟨R_int⟩=0.009 | steps= 57 | loss_w=-0.100 | loss_m_pg=-0.047\n",
      "EP 00250 | ⟨R_ext⟩=0.091 | ⟨R_int⟩=0.012 | steps= 10 | loss_w=-0.282 | loss_m_pg=+0.000\n",
      "EP 00260 | ⟨R_ext⟩=0.013 | ⟨R_int⟩=0.008 | steps= 46 | loss_w=-0.065 | loss_m_pg=+0.024\n",
      "EP 00270 | ⟨R_ext⟩=0.036 | ⟨R_int⟩=0.000 | steps= 22 | loss_w=-0.132 | loss_m_pg=+0.034\n",
      "EP 00280 | ⟨R_ext⟩=0.068 | ⟨R_int⟩=0.005 | steps= 13 | loss_w=-0.055 | loss_m_pg=-0.117\n",
      "EP 00290 | ⟨R_ext⟩=0.023 | ⟨R_int⟩=-0.004 | steps= 31 | loss_w=-0.305 | loss_m_pg=+0.073\n",
      "EP 00300 | ⟨R_ext⟩=0.158 | ⟨R_int⟩=0.000 | steps=  6 | loss_w=-0.414 | loss_m_pg=+0.000\n",
      "EP 00310 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.014 | steps=100 | loss_w=+0.236 | loss_m_pg=-0.104\n",
      "EP 00320 | ⟨R_ext⟩=0.191 | ⟨R_int⟩=-0.085 | steps=  5 | loss_w=-0.359 | loss_m_pg=+0.000\n",
      "EP 00330 | ⟨R_ext⟩=0.158 | ⟨R_int⟩=-0.010 | steps=  6 | loss_w=-0.365 | loss_m_pg=+0.000\n",
      "EP 00340 | ⟨R_ext⟩=0.991 | ⟨R_int⟩=0.000 | steps=  1 | loss_w=+0.696 | loss_m_pg=+0.000\n",
      "EP 00350 | ⟨R_ext⟩=0.014 | ⟨R_int⟩=-0.015 | steps= 44 | loss_w=-0.349 | loss_m_pg=-0.026\n",
      "EP 00360 | ⟨R_ext⟩=0.158 | ⟨R_int⟩=-0.089 | steps=  6 | loss_w=-0.435 | loss_m_pg=+0.000\n",
      "EP 00370 | ⟨R_ext⟩=0.011 | ⟨R_int⟩=-0.007 | steps= 51 | loss_w=+0.141 | loss_m_pg=-0.027\n",
      "EP 00380 | ⟨R_ext⟩=0.025 | ⟨R_int⟩=-0.004 | steps= 29 | loss_w=+0.037 | loss_m_pg=+0.006\n",
      "EP 00390 | ⟨R_ext⟩=0.158 | ⟨R_int⟩=0.046 | steps=  6 | loss_w=+0.687 | loss_m_pg=+0.000\n",
      "EP 00400 | ⟨R_ext⟩=0.008 | ⟨R_int⟩=0.009 | steps= 59 | loss_w=-0.085 | loss_m_pg=+0.022\n",
      "EP 00410 | ⟨R_ext⟩=0.014 | ⟨R_int⟩=0.010 | steps= 43 | loss_w=-0.082 | loss_m_pg=+0.040\n",
      "EP 00420 | ⟨R_ext⟩=0.034 | ⟨R_int⟩=-0.011 | steps= 23 | loss_w=-0.472 | loss_m_pg=-0.014\n",
      "EP 00430 | ⟨R_ext⟩=0.007 | ⟨R_int⟩=-0.022 | steps= 61 | loss_w=-0.012 | loss_m_pg=+0.057\n",
      "EP 00440 | ⟨R_ext⟩=0.015 | ⟨R_int⟩=0.005 | steps= 42 | loss_w=-0.039 | loss_m_pg=+0.054\n",
      "EP 00450 | ⟨R_ext⟩=0.041 | ⟨R_int⟩=-0.018 | steps= 20 | loss_w=-0.286 | loss_m_pg=+0.022\n",
      "EP 00460 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.003 | steps=100 | loss_w=+0.108 | loss_m_pg=-0.098\n",
      "EP 00470 | ⟨R_ext⟩=0.025 | ⟨R_int⟩=0.015 | steps= 29 | loss_w=-0.084 | loss_m_pg=+0.012\n",
      "EP 00480 | ⟨R_ext⟩=0.324 | ⟨R_int⟩=-0.138 | steps=  3 | loss_w=+0.299 | loss_m_pg=+0.000\n",
      "EP 00490 | ⟨R_ext⟩=0.116 | ⟨R_int⟩=-0.001 | steps=  8 | loss_w=-0.341 | loss_m_pg=+0.000\n",
      "EP 00500 | ⟨R_ext⟩=0.028 | ⟨R_int⟩=-0.042 | steps= 27 | loss_w=-0.165 | loss_m_pg=-0.011\n",
      "EP 00510 | ⟨R_ext⟩=0.047 | ⟨R_int⟩=-0.030 | steps= 18 | loss_w=-0.043 | loss_m_pg=+0.002\n",
      "EP 00520 | ⟨R_ext⟩=0.022 | ⟨R_int⟩=0.001 | steps= 32 | loss_w=-0.416 | loss_m_pg=+0.028\n",
      "EP 00530 | ⟨R_ext⟩=0.010 | ⟨R_int⟩=-0.013 | steps= 54 | loss_w=-0.165 | loss_m_pg=+0.075\n",
      "EP 00540 | ⟨R_ext⟩=0.020 | ⟨R_int⟩=-0.010 | steps= 35 | loss_w=-0.173 | loss_m_pg=+0.040\n",
      "EP 00550 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.006 | steps=100 | loss_w=+0.075 | loss_m_pg=-0.071\n",
      "EP 00560 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.001 | steps=100 | loss_w=+0.145 | loss_m_pg=+0.024\n",
      "EP 00570 | ⟨R_ext⟩=0.134 | ⟨R_int⟩=-0.007 | steps=  7 | loss_w=-0.418 | loss_m_pg=+0.000\n",
      "EP 00580 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.010 | steps=100 | loss_w=+0.010 | loss_m_pg=-0.039\n",
      "EP 00590 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.001 | steps=100 | loss_w=-0.092 | loss_m_pg=-0.017\n",
      "EP 00600 | ⟨R_ext⟩=0.002 | ⟨R_int⟩=-0.026 | steps= 95 | loss_w=-0.196 | loss_m_pg=+0.049\n",
      "EP 00610 | ⟨R_ext⟩=0.004 | ⟨R_int⟩=-0.037 | steps= 75 | loss_w=-0.337 | loss_m_pg=+0.029\n",
      "EP 00620 | ⟨R_ext⟩=0.011 | ⟨R_int⟩=-0.017 | steps= 51 | loss_w=-0.296 | loss_m_pg=+0.005\n",
      "EP 00630 | ⟨R_ext⟩=0.091 | ⟨R_int⟩=-0.018 | steps= 10 | loss_w=-0.103 | loss_m_pg=+0.000\n",
      "EP 00640 | ⟨R_ext⟩=0.008 | ⟨R_int⟩=-0.038 | steps= 60 | loss_w=+0.025 | loss_m_pg=-0.005\n",
      "EP 00650 | ⟨R_ext⟩=0.022 | ⟨R_int⟩=-0.029 | steps= 32 | loss_w=-0.223 | loss_m_pg=+0.006\n",
      "EP 00660 | ⟨R_ext⟩=0.191 | ⟨R_int⟩=-0.045 | steps=  5 | loss_w=-0.548 | loss_m_pg=+0.000\n",
      "EP 00670 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.008 | steps=100 | loss_w=-0.083 | loss_m_pg=-0.002\n",
      "EP 00680 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.012 | steps=100 | loss_w=-0.023 | loss_m_pg=-0.102\n",
      "EP 00690 | ⟨R_ext⟩=0.008 | ⟨R_int⟩=-0.047 | steps= 58 | loss_w=-0.166 | loss_m_pg=-0.018\n",
      "EP 00700 | ⟨R_ext⟩=0.991 | ⟨R_int⟩=0.000 | steps=  1 | loss_w=+1.134 | loss_m_pg=+0.000\n",
      "EP 00710 | ⟨R_ext⟩=0.006 | ⟨R_int⟩=-0.014 | steps= 66 | loss_w=-0.170 | loss_m_pg=+0.084\n",
      "EP 00720 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.005 | steps=100 | loss_w=+0.537 | loss_m_pg=-0.003\n",
      "EP 00730 | ⟨R_ext⟩=0.058 | ⟨R_int⟩=0.045 | steps= 15 | loss_w=-0.205 | loss_m_pg=+0.159\n",
      "EP 00740 | ⟨R_ext⟩=0.054 | ⟨R_int⟩=-0.014 | steps= 16 | loss_w=-0.297 | loss_m_pg=-0.045\n",
      "EP 00750 | ⟨R_ext⟩=0.134 | ⟨R_int⟩=-0.017 | steps=  7 | loss_w=-0.242 | loss_m_pg=+0.000\n",
      "EP 00760 | ⟨R_ext⟩=0.007 | ⟨R_int⟩=-0.047 | steps= 64 | loss_w=-0.170 | loss_m_pg=+0.005\n",
      "EP 00770 | ⟨R_ext⟩=0.034 | ⟨R_int⟩=-0.006 | steps= 23 | loss_w=-0.250 | loss_m_pg=+0.067\n",
      "EP 00780 | ⟨R_ext⟩=0.054 | ⟨R_int⟩=-0.029 | steps= 16 | loss_w=-0.218 | loss_m_pg=-0.031\n",
      "EP 00790 | ⟨R_ext⟩=0.023 | ⟨R_int⟩=-0.040 | steps= 31 | loss_w=-0.323 | loss_m_pg=+0.034\n",
      "EP 00800 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.002 | steps=100 | loss_w=+0.107 | loss_m_pg=-0.002\n",
      "EP 00810 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.000 | steps=100 | loss_w=+0.181 | loss_m_pg=-0.006\n",
      "EP 00820 | ⟨R_ext⟩=0.324 | ⟨R_int⟩=-0.021 | steps=  3 | loss_w=-0.600 | loss_m_pg=+0.000\n",
      "EP 00830 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.009 | steps=100 | loss_w=+0.182 | loss_m_pg=-0.029\n",
      "EP 00840 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.010 | steps=100 | loss_w=+0.162 | loss_m_pg=-0.022\n",
      "EP 00850 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.000 | steps=100 | loss_w=+0.444 | loss_m_pg=-0.000\n",
      "EP 00860 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.023 | steps=100 | loss_w=+0.091 | loss_m_pg=-0.071\n",
      "EP 00870 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.001 | steps=100 | loss_w=+0.141 | loss_m_pg=-0.005\n",
      "EP 00880 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.001 | steps=100 | loss_w=+0.177 | loss_m_pg=+0.003\n",
      "EP 00890 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.008 | steps=100 | loss_w=+0.222 | loss_m_pg=-0.023\n",
      "EP 00900 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.001 | steps=100 | loss_w=+0.270 | loss_m_pg=-0.004\n",
      "EP 00910 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.000 | steps=100 | loss_w=+0.185 | loss_m_pg=-0.000\n",
      "EP 00920 | ⟨R_ext⟩=0.324 | ⟨R_int⟩=0.001 | steps=  3 | loss_w=-0.439 | loss_m_pg=+0.000\n",
      "EP 00930 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.001 | steps=100 | loss_w=-0.122 | loss_m_pg=-0.002\n",
      "EP 00940 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.002 | steps=100 | loss_w=+0.080 | loss_m_pg=+0.007\n",
      "EP 00950 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.005 | steps=100 | loss_w=+0.089 | loss_m_pg=+0.018\n",
      "EP 00960 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.002 | steps=100 | loss_w=-0.272 | loss_m_pg=-0.007\n",
      "EP 00970 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.001 | steps=100 | loss_w=+0.291 | loss_m_pg=-0.002\n",
      "EP 00980 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.002 | steps=100 | loss_w=-0.084 | loss_m_pg=-0.005\n",
      "EP 00990 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.003 | steps=100 | loss_w=+0.166 | loss_m_pg=-0.010\n",
      "EP 01000 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.004 | steps=100 | loss_w=+0.259 | loss_m_pg=+0.015\n",
      "EP 01010 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.001 | steps=100 | loss_w=-0.042 | loss_m_pg=-0.013\n",
      "EP 01020 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.001 | steps=100 | loss_w=-0.225 | loss_m_pg=-0.018\n",
      "EP 01030 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.004 | steps=100 | loss_w=-0.061 | loss_m_pg=+0.005\n",
      "EP 01040 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.000 | steps=100 | loss_w=+0.037 | loss_m_pg=-0.000\n",
      "EP 01050 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.004 | steps=100 | loss_w=-0.258 | loss_m_pg=-0.018\n",
      "EP 01060 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.008 | steps=100 | loss_w=-0.158 | loss_m_pg=-0.028\n",
      "EP 01070 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.001 | steps=100 | loss_w=+0.049 | loss_m_pg=-0.004\n",
      "EP 01080 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.004 | steps=100 | loss_w=-0.224 | loss_m_pg=-0.018\n",
      "EP 01090 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.003 | steps=100 | loss_w=-0.043 | loss_m_pg=+0.010\n",
      "EP 01100 | ⟨R_ext⟩=0.491 | ⟨R_int⟩=0.000 | steps=  2 | loss_w=-0.467 | loss_m_pg=+0.000\n",
      "EP 01110 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.005 | steps=100 | loss_w=+0.215 | loss_m_pg=+0.013\n",
      "EP 01120 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.000 | steps=100 | loss_w=+0.028 | loss_m_pg=-0.000\n",
      "EP 01130 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.000 | steps=100 | loss_w=+0.017 | loss_m_pg=-0.000\n",
      "EP 01140 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.007 | steps=100 | loss_w=-0.045 | loss_m_pg=-0.022\n",
      "EP 01150 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.008 | steps=100 | loss_w=-0.069 | loss_m_pg=-0.008\n",
      "EP 01160 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.007 | steps=100 | loss_w=-0.024 | loss_m_pg=+0.002\n",
      "EP 01170 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.006 | steps=100 | loss_w=-0.002 | loss_m_pg=+0.014\n",
      "EP 01180 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.000 | steps=100 | loss_w=+0.034 | loss_m_pg=-0.000\n",
      "EP 01190 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.003 | steps=100 | loss_w=+0.150 | loss_m_pg=-0.010\n",
      "EP 01200 | ⟨R_ext⟩=0.491 | ⟨R_int⟩=0.000 | steps=  2 | loss_w=-0.410 | loss_m_pg=+0.000\n",
      "EP 01210 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.003 | steps=100 | loss_w=-0.024 | loss_m_pg=-0.020\n",
      "EP 01220 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.003 | steps=100 | loss_w=-0.088 | loss_m_pg=+0.012\n",
      "EP 01230 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.000 | steps=100 | loss_w=+0.045 | loss_m_pg=-0.000\n",
      "EP 01240 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=0.000 | steps=100 | loss_w=+0.031 | loss_m_pg=-0.000\n",
      "EP 01250 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.002 | steps=100 | loss_w=-0.135 | loss_m_pg=-0.008\n",
      "EP 01260 | ⟨R_ext⟩=0.000 | ⟨R_int⟩=-0.003 | steps=100 | loss_w=-0.034 | loss_m_pg=-0.013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 193\u001b[0m\n\u001b[0;32m    190\u001b[0m r_exts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[1;32m--> 193\u001b[0m     logits, s_t, g_t \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(logits\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    195\u001b[0m     a \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\luk27\\anaconda3\\envs\\interp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luk27\\anaconda3\\envs\\interp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 102\u001b[0m, in \u001b[0;36mFuNAgent.forward\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, t: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    101\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpercept(x)\n\u001b[1;32m--> 102\u001b[0m     s_t, g_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.05\u001b[39m:\n\u001b[0;32m    104\u001b[0m         noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(g_t, device\u001b[38;5;241m=\u001b[39mg_t\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\luk27\\anaconda3\\envs\\interp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luk27\\anaconda3\\envs\\interp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 55\u001b[0m, in \u001b[0;36mManager.forward\u001b[1;34m(self, z, t)\u001b[0m\n\u001b[0;32m     53\u001b[0m s \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspace(z))\n\u001b[0;32m     54\u001b[0m g_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(s, t)\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luk27\\anaconda3\\envs\\interp\\Lib\\site-packages\\torch\\nn\\functional.py:5373\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[0;32m   5369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   5370\u001b[0m         normalize, (\u001b[38;5;28minput\u001b[39m, out), \u001b[38;5;28minput\u001b[39m, p\u001b[38;5;241m=\u001b[39mp, dim\u001b[38;5;241m=\u001b[39mdim, eps\u001b[38;5;241m=\u001b[39meps, out\u001b[38;5;241m=\u001b[39mout\n\u001b[0;32m   5371\u001b[0m     )\n\u001b[0;32m   5372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 5373\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclamp_min(eps)\u001b[38;5;241m.\u001b[39mexpand_as(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   5374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m/\u001b[39m denom\n\u001b[0;32m   5375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luk27\\anaconda3\\envs\\interp\\Lib\\site-packages\\torch\\_tensor.py:827\u001b[0m, in \u001b[0;36mTensor.norm\u001b[1;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    825\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mnorm, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, p\u001b[38;5;241m=\u001b[39mp, dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39mkeepdim, dtype\u001b[38;5;241m=\u001b[39mdtype\n\u001b[0;32m    826\u001b[0m     )\n\u001b[1;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luk27\\anaconda3\\envs\\interp\\Lib\\site-packages\\torch\\functional.py:1822\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1820\u001b[0m _p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m p\n\u001b[0;32m   1821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28minput\u001b[39m, _p, _dim, keepdim, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout\n\u001b[0;32m   1826\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "import gym, numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "# ───────────────────────── FuN building blocks ──────────────────────────\n",
    "class DilatedLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, dilation: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size, self.dilation = hidden_size, dilation\n",
    "        self.cell = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.register_buffer(\"_h\", torch.zeros(dilation, 1, hidden_size))\n",
    "        self.register_buffer(\"_c\", torch.zeros_like(self._h))\n",
    "\n",
    "    def reset(self, batch: int, device: torch.device):\n",
    "        self._h = torch.zeros(self.dilation, batch, self.hidden_size, device=device)\n",
    "        self._c = torch.zeros_like(self._h)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: int):\n",
    "        i = t % self.dilation\n",
    "        h, c = self.cell(x, (self._h[i], self._c[i]))\n",
    "        self._h = self._h.clone()\n",
    "        self._c = self._c.clone()\n",
    "        self._h[i] = h\n",
    "        self._c[i] = c\n",
    "        return h\n",
    "\n",
    "\n",
    "class Perception(nn.Module):\n",
    "    def __init__(self, percept_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 3, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, percept_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.encoder(x.view(x.size(0), -1))  # Flatten the grid\n",
    "\n",
    "\n",
    "\n",
    "class Manager(nn.Module):\n",
    "    def __init__(self, percept_size: int, state_size: int, dilation: int):\n",
    "        super().__init__()\n",
    "        self.space = nn.Linear(percept_size, state_size)\n",
    "        self.rnn = DilatedLSTM(state_size, state_size, dilation)\n",
    "\n",
    "    def reset(self, batch: int, device: torch.device):\n",
    "        self.rnn.reset(batch, device)\n",
    "\n",
    "    def forward(self, z: torch.Tensor, t: int):\n",
    "        s = F.relu(self.space(z))\n",
    "        g_hat = self.rnn(s, t)\n",
    "        return s, F.normalize(g_hat, dim=-1)\n",
    "\n",
    "\n",
    "class Worker(nn.Module):\n",
    "    def __init__(self, percept_size: int, action_n: int, state_size: int, goal_size: int):\n",
    "        super().__init__()\n",
    "        self.goal_size, self.action_n = goal_size, action_n\n",
    "        self.goal_embed = nn.Linear(state_size, goal_size, bias=False)\n",
    "        self.rnn = nn.LSTMCell(percept_size, goal_size * action_n)\n",
    "        self.register_buffer(\"_h\", torch.zeros(1, goal_size * action_n))\n",
    "        self.register_buffer(\"_c\", torch.zeros_like(self._h))\n",
    "\n",
    "    def reset(self, batch: int, device: torch.device):\n",
    "        shape = (batch, self.goal_size * self.action_n)\n",
    "        self._h = torch.zeros(*shape, device=device)\n",
    "        self._c = torch.zeros_like(self._h)\n",
    "\n",
    "    def forward(self, z: torch.Tensor, goal_sum: torch.Tensor):\n",
    "        h, c = self.rnn(z, (self._h, self._c))\n",
    "        self._h, self._c = h, c\n",
    "        U = h.view(-1, self.goal_size, self.action_n)\n",
    "        w = self.goal_embed(goal_sum)\n",
    "        logits = (w.unsqueeze(1) @ U).squeeze(1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class FuNAgent(nn.Module):\n",
    "    def __init__(self, percept_size=128, hidden=256, state=64,\n",
    "                 goal=8, dilation=10, action_n=7):\n",
    "        super().__init__()\n",
    "        self.dilation = dilation\n",
    "        self.percept = Perception(percept_size, hidden)\n",
    "        self.manager = Manager(percept_size, state, dilation)\n",
    "        self.worker = Worker(percept_size, action_n, state, goal)\n",
    "\n",
    "        self.worker_v = nn.Sequential(nn.Linear(state, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "        self.manager_v = nn.Sequential(nn.Linear(state, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "\n",
    "        self._g_queue: deque[torch.Tensor] = deque(maxlen=dilation)\n",
    "\n",
    "    def reset(self, batch: int, device: torch.device):\n",
    "        self._g_queue.clear()\n",
    "        self.manager.reset(batch, device)\n",
    "        self.worker.reset(batch, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: int):\n",
    "        z = self.percept(x)\n",
    "        s_t, g_t = self.manager(z, t)\n",
    "        if torch.rand(1).item() < 0.05:\n",
    "            noise = torch.randn_like(g_t, device=g_t.device)\n",
    "            g_t = F.normalize(noise, dim=-1)\n",
    "        self._g_queue.append(g_t)\n",
    "        g_sum = torch.stack(tuple(self._g_queue)).sum(0)\n",
    "        logits = self.worker(z, g_sum)\n",
    "        return logits, s_t, g_t\n",
    "\n",
    "\n",
    "def discounted_cumsum(x: torch.Tensor, gamma: float) -> torch.Tensor:\n",
    "    out = torch.empty_like(x, device=x.device)\n",
    "    G = torch.tensor(0.0, device=x.device)\n",
    "    for t in reversed(range(len(x))):\n",
    "        G = x[t] + gamma * G\n",
    "        out[t] = G\n",
    "    return out\n",
    "\n",
    "\n",
    "def preprocess(frame: np.ndarray) -> torch.Tensor:\n",
    "    frame = torch.tensor(frame, dtype=torch.float32) / 10.0  # Normalize discrete values\n",
    "    return frame.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "\n",
    "from gym import Wrapper\n",
    "\n",
    "class CustomUnlockRewardWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.key_picked = False\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.key_picked = False\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        try:\n",
    "            if not self.key_picked and self.env.carrying and self.env.carrying.type == \"key\":\n",
    "                reward = 1.0\n",
    "                self.key_picked = True\n",
    "            elif terminated and self.env.door and self.env.door.is_open:\n",
    "                reward = 1.0\n",
    "            else:\n",
    "                reward = 0.0\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "# ─────────────────────────── main loop ────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "ENV_ID = \"MiniGrid-Empty-Random-5x5-v0\"#\"MiniGrid-Unlock-v0\"\n",
    "NUM_EPISODES = 20_000\n",
    "DILATION = 10\n",
    "HORIZON = DILATION\n",
    "GAMMA_EXT = 0.99\n",
    "GAMMA_INT = 0.90\n",
    "INTRINSIC_SCALE = 0.1\n",
    "LR_MANAGER = 3e-4\n",
    "LR_WORKER = 3e-4\n",
    "CLIP_NORM = 40.0\n",
    "\n",
    "env = CustomUnlockRewardWrapper(gym.make(ENV_ID, render_mode=\"rgb_array\"))\n",
    "action_n = env.action_space.n\n",
    "agent = FuNAgent(dilation=DILATION, action_n=action_n).to(device)\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {\"params\": list(agent.percept.parameters()) +\n",
    "               list(agent.worker.parameters()) +\n",
    "               list(agent.worker_v.parameters()),\n",
    "     \"lr\": LR_WORKER},\n",
    "    {\"params\": list(agent.manager.parameters()) +\n",
    "               list(agent.manager_v.parameters()),\n",
    "     \"lr\": LR_MANAGER},\n",
    "])\n",
    "\n",
    "for ep in range(1, NUM_EPISODES + 1):\n",
    "    obs, _ = env.reset()\n",
    "    img = preprocess(obs[\"image\"])\n",
    "    agent.reset(batch=1, device=device)\n",
    "\n",
    "    states, goals = [], []\n",
    "    logps, entrs = [], []\n",
    "    r_exts = []\n",
    "\n",
    "    for t in count():\n",
    "        logits, s_t, g_t = agent(img, t)\n",
    "        dist = torch.distributions.Categorical(logits=logits.squeeze(0))\n",
    "        a = dist.sample()\n",
    "        logps.append(dist.log_prob(a))\n",
    "        entrs.append(dist.entropy())\n",
    "\n",
    "        obs, r_ext, term, trunc, _ = env.step(a.item())\n",
    "        img = preprocess(obs[\"image\"])\n",
    "\n",
    "        states.append(s_t.squeeze(0))\n",
    "        goals.append(g_t.squeeze(0))\n",
    "        r_exts.append(torch.tensor(r_ext, dtype=torch.float32, device=device))\n",
    "\n",
    "        if term or trunc:\n",
    "            break\n",
    "\n",
    "    s = torch.stack(states)\n",
    "    g = torch.stack(goals)\n",
    "    logp = torch.stack(logps)\n",
    "    entr = torch.stack(entrs)\n",
    "    r_ext = torch.stack(r_exts)\n",
    "    T = s.size(0)\n",
    "\n",
    "    r_int = torch.zeros(T, device=device)\n",
    "    for t in range(T):\n",
    "        h = min(HORIZON, t)\n",
    "        if h > 0:\n",
    "            diff = s[t].unsqueeze(0) - s[t - h:t]\n",
    "            sim = F.cosine_similarity(diff, g[t - h:t], dim=-1)\n",
    "            r_int[t] = sim.mean()\n",
    "\n",
    "    R_ext = discounted_cumsum(r_ext, GAMMA_EXT)\n",
    "    R_int = discounted_cumsum(r_int, GAMMA_INT)\n",
    "    R_tot = R_ext + INTRINSIC_SCALE * R_int\n",
    "\n",
    "    V_w = agent.worker_v(s).squeeze(-1)\n",
    "    V_m = agent.manager_v(s).squeeze(-1)\n",
    "\n",
    "    adv_w = (R_tot - V_w.detach())\n",
    "    if len(adv_w) > 1:\n",
    "        adv_w = (adv_w - adv_w.mean()) / (adv_w.std() + 1e-8)\n",
    "\n",
    "    adv_m = (R_ext - V_m.detach())\n",
    "    if len(adv_m) > 1:\n",
    "        adv_m = (adv_m - adv_m.mean()) / (adv_m.std() + 1e-8)\n",
    "\n",
    "    entropy = entr.mean()\n",
    "    pg_w = -(adv_w * logp).mean()\n",
    "    v_w_loss = 0.5 * F.mse_loss(V_w, R_tot.detach())\n",
    "    loss_w = pg_w + v_w_loss - 0.01 * entropy\n",
    "\n",
    "    if T > HORIZON:\n",
    "        trans_cos = F.cosine_similarity(\n",
    "            (s[HORIZON:] - s[:-HORIZON]).detach(), g[:-HORIZON], dim=-1)\n",
    "        loss_m_pg = -(adv_m[:-HORIZON] * trans_cos).mean()\n",
    "    else:\n",
    "        loss_m_pg = torch.zeros((), device=device)\n",
    "\n",
    "    v_m_loss = 0.5 * F.mse_loss(V_m, R_ext.detach())\n",
    "    loss_m = loss_m_pg + v_m_loss\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    (loss_w + loss_m).backward()\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), CLIP_NORM)\n",
    "    optimizer.step()\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        print(f\"EP {ep:05d} | \"\n",
    "              f\"⟨R_ext⟩={r_ext.float().mean():.3f} | \"\n",
    "              f\"⟨R_int⟩={r_int.float().mean():.3f} | \"\n",
    "              f\"steps={T:3d} | \"\n",
    "              f\"loss_w={loss_w.item():+.3f} | \"\n",
    "              f\"loss_m_pg={loss_m_pg.item():+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eca5051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving all_episodes.mp4...\n",
      "MP4 video saved.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import gym\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def display_episodes(agent, preprocess, env_id=ENV_ID, num_episodes=10, max_steps=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent.to(device)\n",
    "\n",
    "    all_frames = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "        obs, _ = env.reset()\n",
    "        img = preprocess(obs[\"image\"]).to(device)\n",
    "        agent.reset(batch=1, device=device)\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            frame = env.render()\n",
    "\n",
    "            all_frames.append(np.array(frame))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, *_ = agent(img, t)\n",
    "                action = torch.distributions.Categorical(logits=logits.squeeze(0)).sample().item()\n",
    "\n",
    "            obs, _, terminated, truncated, _ = env.step(action)\n",
    "            img = preprocess(obs[\"image\"]).to(device)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    video_path = \"all_episodes.mp4\"\n",
    "    print(f\"Saving {video_path}...\")\n",
    "\n",
    "    # Save as MP4 video\n",
    "    with imageio.get_writer(video_path, fps=5, codec='libx264', format='mp4') as writer:\n",
    "        for frame in all_frames:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "    print(\"MP4 video saved.\")\n",
    "\n",
    "# Usage:\n",
    "display_episodes(agent, preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cba6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
